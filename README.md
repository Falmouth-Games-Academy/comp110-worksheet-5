# comp110-worksheet-C
Base repository for COMP110 worksheet C

a. This algorithm performs a duplicate check of a single list. It iterates through the list, checking every other part of the list to see if it is not in the same place and if it is equal

b. Becuase the length is I*J where I and J are the length of the list i.e O(n^2)

c. Every point is still being checked but only once instead of twice, and this speeds it up too.

d. Because everything is being checked once instead of twice the algorithm is almost twice as fast

e. Yes. On average j has halved I*(J/2) --> I*J which again is O(n^2)

f. O(nlog(n)) https://stackoverflow.com/questions/14434490/what-is-the-complexity-of-this-python-sort-method

g. the sort is O(nlog(n)+n) --> so the sort is O(nlog(n))

h. the second algorithm would be MUCH faster with a large data sort. due to O(nlog(n)) being faster than O(n^2)

i. slower algoriths are often much faster to implement so with small data sets this often may not matter
